{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax, jit, grad, random, vmap, pmap, local_device_count, tree_map, jacfwd, jacrev\n",
    "from jax.tree_util import tree_map, tree_reduce, tree_leaves\n",
    "from functools import partial\n",
    "from absl import logging\n",
    "import os\n",
    "import time\n",
    "import ml_collections\n",
    "import jax\n",
    "from jax.tree_util import tree_map\n",
    "from flax.training import train_state\n",
    "from flax import jax_utils\n",
    "from typing import Any, Callable, Sequence, Tuple, Optional, Dict\n",
    "from matplotlib import pyplot as plt\n",
    "from jaxpi.evaluator import BaseEvaluator\n",
    "\n",
    "\n",
    "import optax\n",
    "\n",
    "from jaxpi import archs\n",
    "from jaxpi.utils import flatten_pytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 1.0\n",
    "t_0 = 0.0\n",
    "t_end = 50.0\n",
    "r = 1000.0\n",
    "n_samples = 50\n",
    "c = 0.01\n",
    "\n",
    "def solution(t):\n",
    "    return - t / (r*c) + jnp.log(u/r)\n",
    "\n",
    "def get_dataset():\n",
    "    t = jnp.linspace(t_0, t_end, n_samples)\n",
    "    u = solution(t)\n",
    "    return t,u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler():\n",
    "    #@partial(pmap, static_broadcasted_argnums=(0,))\n",
    "    def data_generation():\n",
    "        dom = jnp.array([[0., 50.]])\n",
    "        dim = dom.shape[0]\n",
    "\n",
    "        batch = random.uniform(\n",
    "            random.PRNGKey(1234),\n",
    "            shape=(5, dim),\n",
    "            minval=dom[:, 0],\n",
    "            maxval=dom[:, 1],\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    weights: Dict\n",
    "    momentum: float\n",
    "\n",
    "    def apply_weights(self, weights, **kwargs):\n",
    "        \"\"\"Updates `weights` using running average  in return value.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with new weights updated by applying `running_average`,\n",
    "          and additional attributes replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "\n",
    "        running_average = (\n",
    "            lambda old_w, new_w: old_w * self.momentum + (1 - self.momentum) * new_w\n",
    "        )\n",
    "        weights = tree_map(running_average, self.weights, weights)\n",
    "        weights = lax.stop_gradient(weights)\n",
    "\n",
    "        return self.replace(\n",
    "            step=self.step,\n",
    "            params=self.params,\n",
    "            opt_state=self.opt_state,\n",
    "            weights=weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def _create_arch(config):\n",
    "    if config.arch_name == \"Mlp\":\n",
    "        arch = archs.Mlp(**config)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Arch {config.arch_name} not supported yet!\")\n",
    "\n",
    "    return arch\n",
    "\n",
    "\n",
    "def _create_optimizer(config):\n",
    "    if config.optimizer == \"Adam\":\n",
    "        lr = optax.exponential_decay(\n",
    "            init_value=config.learning_rate,\n",
    "            transition_steps=config.decay_steps,\n",
    "            decay_rate=config.decay_rate,\n",
    "        )\n",
    "        tx = optax.adam(\n",
    "            learning_rate=lr, b1=config.beta1, b2=config.beta2, eps=config.eps\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {config.optimizer} not supported yet!\")\n",
    "\n",
    "    # Gradient accumulation\n",
    "    if config.grad_accum_steps > 1:\n",
    "        tx = optax.MultiSteps(tx, every_k_schedule=config.grad_accum_steps)\n",
    "\n",
    "    return tx\n",
    "\n",
    "# --- ok\n",
    "# Create nn module from config file\n",
    "def _create_train_state(config):\n",
    "    # Initialize network\n",
    "    arch = _create_arch(config.arch) # nn.module\n",
    "    x = jnp.ones(config.input_dim)\n",
    "    params = arch.init(random.PRNGKey(config.seed), x)\n",
    "\n",
    "    # Initialize optax optimizer\n",
    "    tx = _create_optimizer(config.optim)\n",
    "\n",
    "    # Convert config dict to dict\n",
    "    init_weights = dict(config.weighting.init_weights)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=arch.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        weights=init_weights,\n",
    "        momentum=config.weighting.momentum,\n",
    "    )\n",
    "\n",
    "    return jax_utils.replicate(state)\n",
    "\n",
    "\n",
    "class PINN:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.state = _create_train_state(config)\n",
    "\n",
    "    def u_net(self, params, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def r_net(self, params, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def losses(self, params, batch, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def compute_diag_ntk(self, params, batch, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, weights, batch, *args):\n",
    "        # Compute losses\n",
    "        losses = self.losses(params, batch, *args)\n",
    "        # Compute weighted loss\n",
    "        weighted_losses = tree_map(lambda x, y: x * y, losses, weights)\n",
    "        # Sum weighted losses\n",
    "        loss = tree_reduce(lambda x, y: x + y, weighted_losses)\n",
    "        return loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_weights(self, params, batch, *args):\n",
    "        if self.config.weighting.scheme == \"grad_norm\":\n",
    "            # Compute the gradient of each loss w.r.t. the parameters\n",
    "            grads = jacrev(self.losses)(params, batch, *args)\n",
    "\n",
    "            # Compute the grad norm of each loss\n",
    "            grad_norm_dict = {}\n",
    "            for key, value in grads.items():\n",
    "                flattened_grad = flatten_pytree(value)\n",
    "                grad_norm_dict[key] = jnp.linalg.norm(flattened_grad)\n",
    "\n",
    "            # Compute the mean of grad norms over all losses\n",
    "            mean_grad_norm = jnp.mean(jnp.stack(tree_leaves(grad_norm_dict)))\n",
    "            # Grad Norm Weighting\n",
    "            w = tree_map(lambda x: (mean_grad_norm / x), grad_norm_dict)\n",
    "\n",
    "        return w\n",
    "\n",
    "    @partial(pmap, axis_name=\"batch\", static_broadcasted_argnums=(0,))\n",
    "    def update_weights(self, state, batch, *args):\n",
    "        weights = self.compute_weights(state.params, batch, *args)\n",
    "        weights = lax.pmean(weights, \"batch\")\n",
    "        state = state.apply_weights(weights=weights)\n",
    "        return state\n",
    "\n",
    "    @partial(pmap, axis_name=\"batch\", static_broadcasted_argnums=(0,))\n",
    "    def step(self, state, batch, *args):\n",
    "        grads = grad(self.loss)(state.params, state.weights, batch, *args)\n",
    "        grads = lax.pmean(grads, \"batch\")\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state\n",
    "\n",
    "\n",
    "class ForwardIVP(PINN):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.weighting.use_causal:\n",
    "            self.tol = config.weighting.causal_tol\n",
    "            self.num_chunks = config.weighting.num_chunks\n",
    "            self.M = jnp.triu(jnp.ones((self.num_chunks, self.num_chunks)), k=1).T\n",
    "\n",
    "\n",
    "class ForwardBVP(PINN):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseZero(ForwardIVP):\n",
    "    def __init__(self, config, t_star, u0):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.u0 = u0\n",
    "        self.t_star = t_star\n",
    "\n",
    "        self.t0 = t_star[0]\n",
    "        self.t1 = t_star[-1]\n",
    "\n",
    "        # Predictions over t\n",
    "        self.u_pred_fn = vmap(self.u_net, (0, None)) # -------------- DEBUG\n",
    "        self.r_pred_fn = vmap(self.r_net, (0, None)) # -------------- DEBUG\n",
    "\n",
    "\n",
    "    # Prediction from net for initial value\n",
    "    def u_net(self, params, t):\n",
    "        u = self.state.apply_fn(params, t) # -------------- DEBUG\n",
    "        return u[0]\n",
    "\n",
    "    # Gradient of the neural net\n",
    "    def grad_net(self, params, t):\n",
    "        u_t = grad(self.u_net, argnums=1)(params, t)\n",
    "        return u_t\n",
    "\n",
    "    # Residual of the neural net\n",
    "    def r_net(self, params, t):\n",
    "        u_t = self.grad_net(params, t)\n",
    "        return u_t + 0.1\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def res_and_w(self, params, batch):\n",
    "        \"Compute residuals and weights for causal training\"\n",
    "        # Sort time coordinates\n",
    "        t_sorted = batch[:, 0].sort()\n",
    "        r_pred = vmap(self.r_net, (None, 0))(params, t_sorted) # -------------- DEBUG\n",
    "        # Split residuals into chunks\n",
    "        r_pred = r_pred.reshape(self.num_chunks, -1)\n",
    "        l = jnp.mean(r_pred**2, axis=1)\n",
    "        w = lax.stop_gradient(jnp.exp(-self.tol * (self.M @ l)))\n",
    "        return l, w\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def losses(self, params, batch):\n",
    "        # Initial condition loss\n",
    "        u_pred = self.u_net(params, self.t0) # -------------- DEBUG\n",
    "        ics_loss = jnp.mean((self.u0 - u_pred) ** 2)\n",
    "\n",
    "        # Residual loss\n",
    "        if self.config.weighting.use_causal == True:\n",
    "            l, w = self.res_and_w(params, batch)\n",
    "            res_loss = jnp.mean(l * w)\n",
    "        else:\n",
    "            r_pred = vmap(self.r_net, (None, 0))(params, batch[:, 0]) # -------------- DEBUG\n",
    "            res_loss = jnp.mean((r_pred) ** 2)\n",
    "\n",
    "        loss_dict = {\"ics\": ics_loss, \"res\": res_loss}\n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_l2_error(self, params, u_test):\n",
    "        u_pred = self.u_pred_fn(params, self.t_star)\n",
    "        error = jnp.linalg.norm(u_pred - u_test) / jnp.linalg.norm(u_test)\n",
    "        return error\n",
    "\n",
    "\n",
    "\n",
    "class CaseZeroEvaluator(BaseEvaluator):\n",
    "    def __init__(self, config, model):\n",
    "        super().__init__(config, model)\n",
    "\n",
    "    def log_errors(self, params, u_ref):\n",
    "        l2_error = self.model.compute_l2_error(params, u_ref)\n",
    "        self.log_dict[\"l2_error\"] = l2_error\n",
    "\n",
    "    def log_preds(self, params):\n",
    "        u_pred = self.model.u_pred_fn(params, self.model.t_star) # -------------- DEBUG\n",
    "        fig = plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(u_pred.T, cmap=\"jet\")\n",
    "        self.log_dict[\"u_pred\"] = fig\n",
    "        plt.close()\n",
    "\n",
    "    def __call__(self, state, batch, u_ref):\n",
    "        self.log_dict = super().__call__(state, batch)\n",
    "\n",
    "        if self.config.weighting.use_causal:\n",
    "            _, causal_weight = self.model.res_and_w(state.params, batch)\n",
    "            self.log_dict[\"cas_weight\"] = causal_weight.min()\n",
    "\n",
    "        if self.config.logging.log_errors:\n",
    "            self.log_errors(state.params, u_ref)\n",
    "\n",
    "        if self.config.logging.log_preds:\n",
    "            self.log_preds(state.params)\n",
    "\n",
    "        return self.log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config: ml_collections.ConfigDict, workdir: str):\n",
    "\n",
    "    # Get dataset\n",
    "    t_star, u_ref  = get_dataset()\n",
    "    u0 = u_ref[0]\n",
    "\n",
    "    t0 = t_star[0]\n",
    "    t_end = t_star[-1]\n",
    "\n",
    "    # Define domain\n",
    "    dom = jnp.array([[t0, t_end]])\n",
    "\n",
    "    # Define residual sampler\n",
    "    res_sampler = iter(UniformSampler(dom, config.training.batch_size_per_device))\n",
    "\n",
    "    # Initialize model\n",
    "    model = CaseZero(config, t_star, u0)\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = CaseZeroEvaluator(config, model)\n",
    "\n",
    "    print(\"Waiting for JIT...\")\n",
    "    start_time = time.time()\n",
    "    for step in range(config.training.max_steps):\n",
    "        batch = next(res_sampler)\n",
    "        model.state = model.step(model.state, batch)\n",
    "\n",
    "        if config.weighting.scheme in [\"grad_norm\", \"ntk\"]:\n",
    "            if step % config.weighting.update_every_steps == 0:\n",
    "                model.state = model.update_weights(model.state, batch)\n",
    "\n",
    "        # Log training metrics, only use host 0 to record results\n",
    "        if jax.process_index() == 0:\n",
    "            if step % config.logging.log_every_steps == 0:\n",
    "                # Get the first replica of the state and batch\n",
    "                state = jax.device_get(tree_map(lambda x: x[0], model.state))\n",
    "                batch = jax.device_get(tree_map(lambda x: x[0], batch))\n",
    "                log_dict = evaluator(state, batch, u_ref)\n",
    "\n",
    "                end_time = time.time()\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
