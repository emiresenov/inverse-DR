{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z7/hhdyq2c544q40kc9g6q0kqd00000gn/T/ipykernel_83888/10661796.py:2: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  from jax import lax, jit, grad, random, vmap, pmap, local_device_count, tree_map, jacfwd, jacrev\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax, jit, grad, random, vmap, pmap, local_device_count, tree_map, jacfwd, jacrev\n",
    "from jax.tree_util import tree_map, tree_reduce, tree_leaves\n",
    "from functools import partial\n",
    "from absl import logging\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import os\n",
    "import time\n",
    "import ml_collections\n",
    "import jax\n",
    "from jax.tree_util import tree_map\n",
    "from flax.training import train_state\n",
    "from flax import jax_utils\n",
    "from typing import Any, Callable, Sequence, Tuple, Optional, Dict\n",
    "from matplotlib import pyplot as plt\n",
    "from jaxpi.evaluator import BaseEvaluator\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"  # DETERMINISTIC\n",
    "from ml_collections import config_flags\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import optax\n",
    "\n",
    "from jaxpi import archs\n",
    "from jaxpi.utils import flatten_pytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 1.0\n",
    "t_0 = 0.0\n",
    "t_end = 50.0\n",
    "r = 1000.0\n",
    "n_samples = 50\n",
    "c = 0.01\n",
    "\n",
    "def solution(t):\n",
    "    return - t / (r*c) + jnp.log(u/r)\n",
    "\n",
    "def get_dataset():\n",
    "    t = jnp.linspace(t_0, t_end, n_samples)\n",
    "    u = solution(t)\n",
    "    return t,u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSampler(Dataset):\n",
    "    def __init__(self, batch_size, rng_key=random.PRNGKey(1234)):\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "        self.num_devices = local_device_count()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generate one batch of data\"\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        keys = random.split(subkey, self.num_devices)\n",
    "        batch = self.data_generation(keys)\n",
    "        return batch\n",
    "\n",
    "    def data_generation(self, key):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "class UniformSampler(BaseSampler):\n",
    "    def __init__(self, dom, batch_size, rng_key=random.PRNGKey(1234)):\n",
    "        super().__init__(batch_size, rng_key)\n",
    "        self.dom = dom\n",
    "        self.dim = dom.shape[0]\n",
    "\n",
    "    @partial(pmap, static_broadcasted_argnums=(0,))\n",
    "    def data_generation(self, key):\n",
    "        \"Generates data containing batch_size samples\"\n",
    "        batch = random.uniform(\n",
    "            key,\n",
    "            shape=(self.batch_size, self.dim),\n",
    "            minval=self.dom[:, 0],\n",
    "            maxval=self.dom[:, 1],\n",
    "        )\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    weights: Dict\n",
    "    momentum: float\n",
    "\n",
    "    def apply_weights(self, weights, **kwargs):\n",
    "        \"\"\"Updates `weights` using running average  in return value.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with new weights updated by applying `running_average`,\n",
    "          and additional attributes replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "\n",
    "        running_average = (\n",
    "            lambda old_w, new_w: old_w * self.momentum + (1 - self.momentum) * new_w\n",
    "        )\n",
    "        weights = tree_map(running_average, self.weights, weights)\n",
    "        weights = lax.stop_gradient(weights)\n",
    "\n",
    "        return self.replace(\n",
    "            step=self.step,\n",
    "            params=self.params,\n",
    "            opt_state=self.opt_state,\n",
    "            weights=weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def _create_arch(config):\n",
    "    if config.arch_name == \"Mlp\":\n",
    "        arch = archs.Mlp(**config)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Arch {config.arch_name} not supported yet!\")\n",
    "\n",
    "    return arch\n",
    "\n",
    "\n",
    "def _create_optimizer(config):\n",
    "    if config.optimizer == \"Adam\":\n",
    "        lr = optax.exponential_decay(\n",
    "            init_value=config.learning_rate,\n",
    "            transition_steps=config.decay_steps,\n",
    "            decay_rate=config.decay_rate,\n",
    "        )\n",
    "        tx = optax.adam(\n",
    "            learning_rate=lr, b1=config.beta1, b2=config.beta2, eps=config.eps\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {config.optimizer} not supported yet!\")\n",
    "\n",
    "    # Gradient accumulation\n",
    "    if config.grad_accum_steps > 1:\n",
    "        tx = optax.MultiSteps(tx, every_k_schedule=config.grad_accum_steps)\n",
    "\n",
    "    return tx\n",
    "\n",
    "\n",
    "# Create nn module from config file\n",
    "def _create_train_state(config):\n",
    "    # Initialize network\n",
    "    arch = _create_arch(config.arch) # nn.module\n",
    "    x = jnp.ones(config.input_dim)\n",
    "    params = arch.init(random.PRNGKey(config.seed), x)\n",
    "\n",
    "    # Initialize optax optimizer\n",
    "    tx = _create_optimizer(config.optim)\n",
    "\n",
    "    # Convert config dict to dict\n",
    "    init_weights = dict(config.weighting.init_weights)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=arch.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        weights=init_weights,\n",
    "        momentum=config.weighting.momentum,\n",
    "    )\n",
    "\n",
    "    return jax_utils.replicate(state)\n",
    "\n",
    "\n",
    "class PINN:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.state = _create_train_state(config)\n",
    "\n",
    "    def u_net(self, params, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def r_net(self, params, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def losses(self, params, batch, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def compute_diag_ntk(self, params, batch, *args):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, weights, batch, *args):\n",
    "        # Compute losses\n",
    "        losses = self.losses(params, batch, *args)\n",
    "        # Compute weighted loss\n",
    "        weighted_losses = tree_map(lambda x, y: x * y, losses, weights)\n",
    "        # Sum weighted losses\n",
    "        loss = tree_reduce(lambda x, y: x + y, weighted_losses)\n",
    "        return loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_weights(self, params, batch, *args):\n",
    "        if self.config.weighting.scheme == \"grad_norm\":\n",
    "            # Compute the gradient of each loss w.r.t. the parameters\n",
    "            grads = jacrev(self.losses)(params, batch, *args)\n",
    "\n",
    "            # Compute the grad norm of each loss\n",
    "            grad_norm_dict = {}\n",
    "            for key, value in grads.items():\n",
    "                flattened_grad = flatten_pytree(value)\n",
    "                grad_norm_dict[key] = jnp.linalg.norm(flattened_grad)\n",
    "\n",
    "            # Compute the mean of grad norms over all losses\n",
    "            mean_grad_norm = jnp.mean(jnp.stack(tree_leaves(grad_norm_dict)))\n",
    "            # Grad Norm Weighting\n",
    "            w = tree_map(lambda x: (mean_grad_norm / x), grad_norm_dict)\n",
    "\n",
    "        return w\n",
    "\n",
    "    @partial(pmap, axis_name=\"batch\", static_broadcasted_argnums=(0,))\n",
    "    def update_weights(self, state, batch, *args):\n",
    "        weights = self.compute_weights(state.params, batch, *args)\n",
    "        weights = lax.pmean(weights, \"batch\")\n",
    "        state = state.apply_weights(weights=weights)\n",
    "        return state\n",
    "\n",
    "    @partial(pmap, axis_name=\"batch\", static_broadcasted_argnums=(0,))\n",
    "    def step(self, state, batch, *args):\n",
    "        grads = grad(self.loss)(state.params, state.weights, batch, *args)\n",
    "        grads = lax.pmean(grads, \"batch\")\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state\n",
    "\n",
    "\n",
    "class ForwardIVP(PINN):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.weighting.use_causal:\n",
    "            self.tol = config.weighting.causal_tol\n",
    "            self.num_chunks = config.weighting.num_chunks\n",
    "            self.M = jnp.triu(jnp.ones((self.num_chunks, self.num_chunks)), k=1).T\n",
    "\n",
    "\n",
    "class ForwardBVP(PINN):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks ok until here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseZero(ForwardIVP):\n",
    "    def __init__(self, config, t_star, u0):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.u0 = u0\n",
    "        self.t_star = t_star\n",
    "\n",
    "        self.t0 = t_star[0]\n",
    "        self.t1 = t_star[-1]\n",
    "\n",
    "        # Predictions over t\n",
    "        self.u_pred_fn = vmap(self.u_net, (0, None)) # -------------- DEBUG\n",
    "        self.r_pred_fn = vmap(self.r_net, (0, None)) # -------------- DEBUG\n",
    "\n",
    "\n",
    "    # Prediction from net for initial value\n",
    "    def u_net(self, params, t):\n",
    "        u = self.state.apply_fn(params, t) # -------------- DEBUG\n",
    "        return u[0]\n",
    "\n",
    "    # Gradient of the neural net\n",
    "    def grad_net(self, params, t):\n",
    "        u_t = grad(self.u_net, argnums=1)(params, t)\n",
    "        return u_t\n",
    "\n",
    "    # Residual of the neural net\n",
    "    def r_net(self, params, t):\n",
    "        u_t = self.grad_net(params, t)\n",
    "        return u_t + 0.1\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def res_and_w(self, params, batch):\n",
    "        \"Compute residuals and weights for causal training\"\n",
    "        # Sort time coordinates\n",
    "        t_sorted = batch[:, 0].sort()\n",
    "        r_pred = vmap(self.r_net, (None, 0))(params, t_sorted) # -------------- DEBUG\n",
    "        # Split residuals into chunks\n",
    "        r_pred = r_pred.reshape(self.num_chunks, -1)\n",
    "        l = jnp.mean(r_pred**2, axis=1)\n",
    "        w = lax.stop_gradient(jnp.exp(-self.tol * (self.M @ l)))\n",
    "        return l, w\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def losses(self, params, batch):\n",
    "        # Initial condition loss\n",
    "        u_pred = self.u_net(params, self.t0) # -------------- DEBUG\n",
    "        ics_loss = jnp.mean((self.u0 - u_pred) ** 2)\n",
    "\n",
    "        # Residual loss\n",
    "        if self.config.weighting.use_causal == True:\n",
    "            l, w = self.res_and_w(params, batch)\n",
    "            res_loss = jnp.mean(l * w)\n",
    "        else:\n",
    "            r_pred = vmap(self.r_net, (None, 0))(params, batch[:, 0]) # -------------- DEBUG\n",
    "            res_loss = jnp.mean((r_pred) ** 2)\n",
    "\n",
    "        loss_dict = {\"ics\": ics_loss, \"res\": res_loss}\n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_l2_error(self, params, u_test):\n",
    "        u_pred = self.u_pred_fn(params, self.t_star)\n",
    "        error = jnp.linalg.norm(u_pred - u_test) / jnp.linalg.norm(u_test)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config: ml_collections.ConfigDict, workdir: str):\n",
    "\n",
    "    # Get dataset\n",
    "    t_star, u_ref  = get_dataset()\n",
    "    u0 = u_ref[0]\n",
    "\n",
    "    t0 = t_star[0]\n",
    "    t_end = t_star[-1]\n",
    "\n",
    "    # Define domain\n",
    "    dom = jnp.array([[t0, t_end]])\n",
    "\n",
    "    # Define residual sampler\n",
    "    res_sampler = iter(UniformSampler(dom, 10))\n",
    "\n",
    "    # Initialize model\n",
    "    model = CaseZero(config, t_star, u0)\n",
    "\n",
    "    print(\"Waiting for JIT...\")\n",
    "    start_time = time.time()\n",
    "    for step in range(config.training.max_steps):\n",
    "        batch = next(res_sampler)\n",
    "        model.state = model.step(model.state, batch)\n",
    "\n",
    "        if config.weighting.scheme in [\"grad_norm\", \"ntk\"]:\n",
    "            if step % config.weighting.update_every_steps == 0:\n",
    "                model.state = model.update_weights(model.state, batch)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    \"\"\"Get the default hyperparameter configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.mode = \"train\"\n",
    "\n",
    "    # Weights & Biases\n",
    "    config.wandb = wandb = ml_collections.ConfigDict()\n",
    "    wandb.project = \"PINN-Burgers\"\n",
    "    wandb.name = \"default_ntk\"\n",
    "    wandb.tag = None\n",
    "\n",
    "    # Arch\n",
    "    config.arch = arch = ml_collections.ConfigDict()\n",
    "    arch.arch_name = \"Mlp\"\n",
    "    arch.num_layers = 4\n",
    "    arch.hidden_dim = 256\n",
    "    arch.out_dim = 1\n",
    "    arch.activation = \"tanh\"\n",
    "\n",
    "    # Optim\n",
    "    config.optim = optim = ml_collections.ConfigDict()\n",
    "    optim.grad_accum_steps = 0\n",
    "    optim.optimizer = \"Adam\"\n",
    "    optim.beta1 = 0.9\n",
    "    optim.beta2 = 0.999\n",
    "    optim.eps = 1e-8\n",
    "    optim.learning_rate = 1e-3\n",
    "    optim.decay_rate = 0.9\n",
    "    optim.decay_steps = 2000\n",
    "\n",
    "    # Training\n",
    "    config.training = training = ml_collections.ConfigDict()\n",
    "    training.max_steps = 2000\n",
    "    training.batch_size_per_device = 4096\n",
    "\n",
    "    # Weighting\n",
    "    config.weighting = weighting = ml_collections.ConfigDict()\n",
    "    weighting.scheme = \"grad_norm\"\n",
    "    weighting.init_weights = ml_collections.ConfigDict({\"ics\": 1.0, \"res\": 1.0})\n",
    "    weighting.momentum = 0.9\n",
    "    weighting.update_every_steps = 1000\n",
    "\n",
    "    weighting.use_causal = False\n",
    "    weighting.causal_tol = 1.0\n",
    "    weighting.num_chunks = 32\n",
    "\n",
    "    # Logging\n",
    "    config.logging = logging = ml_collections.ConfigDict()\n",
    "    logging.log_every_steps = 100\n",
    "    logging.log_errors = True\n",
    "    logging.log_losses = True\n",
    "    logging.log_weights = True\n",
    "    logging.log_preds = False\n",
    "    logging.log_grads = False\n",
    "    logging.log_ntk = False\n",
    "\n",
    "    # Saving\n",
    "    config.saving = saving = ml_collections.ConfigDict()\n",
    "    saving.save_every_steps = None\n",
    "    saving.num_keep_ckpts = 10\n",
    "\n",
    "    # Input shape for initializing Flax models\n",
    "    config.input_dim = 1\n",
    "\n",
    "    # Integer for PRNG random seed.\n",
    "    config.seed = 42\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[ 0.02785836  0.07553934  0.05212061 -0.08060019 -0.08167169 -0.09296588\n",
      "  0.02123309 -0.02695486  0.04823421  0.15635996 -0.00234388  0.06024935\n",
      "  0.11621539 -0.04647679  0.08385476  0.05318992  0.16180624 -0.07660171\n",
      "  0.07738323 -0.10183688 -0.03794707  0.07636463  0.0330898  -0.11297402\n",
      "  0.12117638 -0.08963523 -0.19466987 -0.07570132 -0.05786162 -0.17228028\n",
      " -0.10362356 -0.09207013  0.05888259 -0.00394165 -0.17671101  0.04435476\n",
      " -0.1724185   0.10357135 -0.11694865 -0.172303   -0.03185784 -0.12125486\n",
      "  0.03073543  0.03113442 -0.18074843  0.13842697 -0.05568147  0.01007649\n",
      "  0.02131722 -0.00326253  0.19234365 -0.02868043 -0.10567125 -0.09643216\n",
      " -0.17088675  0.00687698  0.13492474 -0.039799   -0.08398133  0.18455023\n",
      " -0.07093599  0.10274204  0.1407862  -0.09319808  0.05109014 -0.06649379\n",
      "  0.13046506  0.10773941  0.01189771  0.11460226 -0.01935028 -0.05879341\n",
      " -0.11677557 -0.10300769 -0.0209916  -0.0500272  -0.1033225  -0.15880446\n",
      " -0.008666    0.0193164  -0.01819006 -0.09527402  0.03118402  0.07424568\n",
      " -0.0891627  -0.03709853 -0.01145033 -0.16372314  0.00281233 -0.0113246\n",
      "  0.06243842 -0.13275848  0.06736619 -0.19164501 -0.08327638  0.01316625\n",
      "  0.1548157  -0.06411649 -0.05345508 -0.05401237 -0.10658228  0.11511065\n",
      "  0.14265068  0.16381177 -0.04692742 -0.01215     0.06546637 -0.08873644\n",
      " -0.05551369  0.05682693 -0.00033776 -0.12848675  0.05354192  0.17567116\n",
      " -0.00675038 -0.10964523  0.16563156 -0.00586007  0.00440863 -0.09746356\n",
      " -0.14087118 -0.07819226 -0.00878512  0.00292539  0.18735982  0.04866774\n",
      " -0.00462466 -0.1390719   0.14819263  0.04512332  0.03969284  0.03878985\n",
      "  0.02258587  0.01342441 -0.14040647  0.03804196  0.00454518  0.144389\n",
      "  0.02666828  0.11110274  0.00091608  0.11447068  0.04894441 -0.11707915\n",
      " -0.132724    0.08062947  0.18208323 -0.12588736 -0.07094467  0.12969047\n",
      " -0.06850782 -0.07225263  0.02898468  0.06844489 -0.0206997   0.11721469\n",
      " -0.06316919  0.03882956  0.12015118  0.01827276 -0.15309845  0.17233218\n",
      "  0.1068039   0.00819645  0.06303521  0.04146943  0.03165173  0.03993427\n",
      " -0.03815528 -0.04744289 -0.11196202 -0.15569052  0.07113182 -0.06866843\n",
      "  0.05166239  0.01043223 -0.082751    0.13788405  0.1689233   0.02493646\n",
      "  0.0131593   0.04976057  0.03343978 -0.00921691  0.00887795  0.14466293\n",
      " -0.07555322  0.11320695  0.12495409 -0.06724865 -0.06953073 -0.13441977\n",
      " -0.01609885  0.07720255 -0.16820468  0.09529205  0.14293312  0.04045008\n",
      "  0.1676767  -0.12313905 -0.1611654   0.11597359  0.15125006  0.02953744\n",
      " -0.00883342 -0.01157654  0.01310818  0.07215498  0.08319583  0.06717219\n",
      "  0.12949523 -0.14413716 -0.11922072  0.03108575 -0.06278412  0.12686537\n",
      " -0.04425713  0.03342417 -0.00932988  0.00320401  0.01647543  0.06009741\n",
      "  0.06314791  0.15156375 -0.00778998  0.0200348  -0.01906502 -0.12918809\n",
      " -0.02277526 -0.10101061  0.10905711 -0.05131262  0.15510008  0.1071013\n",
      "  0.1311206  -0.07547414  0.03208989  0.18283613  0.00735226 -0.04446649\n",
      "  0.13558115  0.09075926 -0.00071508 -0.07184245 -0.06975711 -0.05880387\n",
      " -0.1174051  -0.09771242 -0.12251729 -0.11383063  0.14250697 -0.0151036\n",
      " -0.06530966  0.07351206  0.00520083  0.18944423]\n",
      "[-0.06779055  0.10901874  0.03052942  0.06854461  0.04144396  0.02010312\n",
      " -0.04673501  0.08582564  0.0956892   0.14325479 -0.01016387  0.10472111\n",
      "  0.05786016 -0.12552652 -0.07342242  0.13914056 -0.06301767  0.04989125\n",
      "  0.00759998  0.00325666 -0.11862011 -0.08006978  0.11028191  0.15671402\n",
      " -0.07589953  0.02078173  0.05682178 -0.17774534  0.02438013 -0.08636556\n",
      "  0.14320609 -0.05396751  0.01448899 -0.0224949   0.16108365 -0.02335441\n",
      " -0.00248506 -0.12645514 -0.05932797 -0.07097385  0.00063658 -0.03191273\n",
      "  0.11080085 -0.21676382 -0.11954291  0.17107551  0.17060928 -0.06604016\n",
      " -0.02071512  0.0642261  -0.01291667  0.0368582  -0.01189781  0.04064176\n",
      " -0.11556299 -0.00883077  0.04676151  0.07990799  0.12407342 -0.04532895\n",
      " -0.13062151 -0.1102989  -0.09937331  0.09046007 -0.0451243   0.0286544\n",
      " -0.03431977  0.1551634   0.15776558  0.00831301 -0.04041903 -0.15010625\n",
      " -0.02147289  0.03465252 -0.08073712 -0.09260523  0.03763825 -0.07922587\n",
      " -0.01936456 -0.06099796  0.03679726  0.04618209  0.07917476  0.04768735\n",
      "  0.13908549 -0.0695461   0.02654149 -0.0959025   0.07202068 -0.02494392\n",
      "  0.26217285  0.00723574  0.22565182  0.08180981 -0.06937104  0.08177735\n",
      "  0.06419969  0.16681704  0.05728223  0.12950525  0.14463344  0.11780604\n",
      " -0.06567544 -0.07094031  0.05754814  0.07261682 -0.00827333  0.04670045\n",
      "  0.13420136  0.03230969  0.04576237  0.05134676  0.01769063 -0.16151066\n",
      "  0.07983346 -0.04923512  0.02037206  0.05735597  0.00303859 -0.00216183\n",
      " -0.04376102 -0.03374958 -0.13928589 -0.09565243 -0.085268   -0.00706214\n",
      "  0.04338699 -0.12651454  0.00268324  0.01627211  0.0962114   0.0087918\n",
      "  0.01976604  0.00408539 -0.01390172  0.0509122  -0.11852869 -0.05146741\n",
      " -0.07854468  0.1766596   0.10244937  0.06668063  0.16825472  0.07096665\n",
      " -0.07558251 -0.0249972  -0.13695103 -0.08221669  0.04486705 -0.01604145\n",
      " -0.07728688  0.0132502  -0.05220762  0.01292134  0.0462954   0.01244352\n",
      " -0.00850391  0.07183798  0.14259644  0.13027956 -0.00585621  0.12059371\n",
      "  0.11312931 -0.11232348  0.02067295 -0.04242129 -0.03907935 -0.03580195\n",
      " -0.07002911 -0.07804538 -0.1138768  -0.03412204  0.03114955  0.08017543\n",
      " -0.00976142  0.06775289 -0.04888783  0.11833476 -0.08430259 -0.14391871\n",
      " -0.08273448 -0.03514773 -0.08699469  0.06811897 -0.02320933  0.1282947\n",
      " -0.16976568  0.05842349  0.06749964  0.05401244  0.00673998  0.16867508\n",
      " -0.0200712   0.07799725 -0.04575565  0.04042748 -0.02657662  0.14897373\n",
      "  0.03648005  0.22106376 -0.12870173 -0.00728399  0.02959568 -0.13890792\n",
      "  0.11381362 -0.04532021  0.21290413 -0.09478774 -0.13458627  0.00502705\n",
      " -0.09583157 -0.09712506 -0.11453875  0.10407    -0.01503564 -0.11597921\n",
      "  0.01752325  0.07882193  0.02853734  0.03734266 -0.02570519  0.09958811\n",
      " -0.03560752  0.0305717   0.05652415 -0.07144739 -0.02066475  0.0611381\n",
      "  0.0892714   0.02097409 -0.06732325 -0.03915668  0.02790646 -0.03244901\n",
      "  0.05322148  0.06582919  0.09712254 -0.08430111  0.01851203  0.08668299\n",
      "  0.18889402  0.01534161 -0.15210398 -0.07667208  0.08150536 -0.0142892\n",
      "  0.1550284  -0.09684188  0.07669806  0.06560536  0.01472685 -0.08931961\n",
      "  0.17735009 -0.05005155  0.08258308  0.14559893]\n",
      "[-2.79280599e-02 -1.93784293e-02 -7.48954564e-02 -8.74991044e-02\n",
      " -1.67971328e-01 -2.17529181e-02 -1.53598160e-01  3.78096625e-02\n",
      "  2.62107328e-02  2.08963570e-03 -3.49171311e-02  1.30850419e-01\n",
      "  6.40427023e-02 -1.07725106e-01 -7.87532628e-02 -2.64648721e-02\n",
      "  4.33526374e-02  3.22127007e-02 -9.96762887e-02 -5.21931909e-02\n",
      "  1.83708817e-01 -6.46088421e-02  5.15271351e-02 -8.76826514e-03\n",
      "  3.32770646e-02 -2.49077100e-02  1.86077878e-02 -9.86795686e-03\n",
      " -9.73558500e-02 -2.26597600e-02 -4.85104844e-02 -9.41173881e-02\n",
      " -4.22553532e-02  1.02504408e-02 -3.47354822e-02  9.55928564e-02\n",
      " -1.84491184e-02  1.09835826e-01 -5.68583645e-02  3.36603411e-02\n",
      " -5.60344607e-02 -1.33713633e-01  1.37566328e-01 -4.99429880e-03\n",
      " -6.41122013e-02 -1.06032111e-01  2.15074364e-02  9.28011611e-02\n",
      "  7.69841224e-02  1.12338230e-01  1.44089200e-02  4.77616563e-02\n",
      " -3.78344301e-03  7.47651309e-02 -2.60723270e-02  5.54082505e-02\n",
      "  1.12713143e-01  6.72570840e-02 -1.00349963e-01 -2.88309827e-02\n",
      " -1.45409480e-01  1.73871184e-03  3.37485969e-02  1.19136922e-01\n",
      " -1.29840255e-01 -2.61915512e-02  1.08222350e-01 -5.30133769e-02\n",
      "  4.92210090e-02 -1.17838465e-01  1.73707798e-01  2.71807499e-02\n",
      "  1.75408244e-01 -9.77219120e-02 -3.65877934e-02  1.10755965e-01\n",
      "  6.70476537e-03 -1.26222670e-01 -2.72204913e-02 -1.49125651e-01\n",
      "  2.11054347e-02  5.21740876e-04 -2.00244319e-02 -6.88080788e-02\n",
      " -1.85997263e-01 -6.55893162e-02  6.66238442e-02  3.04265562e-02\n",
      " -4.60728668e-02  7.20564090e-03  8.28217566e-02  9.68139991e-02\n",
      "  5.36516979e-02 -2.97768917e-02 -2.05723614e-01 -1.59400851e-01\n",
      " -1.05446316e-01 -1.70435733e-03  7.11711682e-03  1.23307377e-01\n",
      " -6.98563904e-02 -1.10316901e-02  4.18397188e-02 -3.64631275e-03\n",
      "  1.84085816e-02  1.91497818e-01 -1.12072498e-01  4.03348245e-02\n",
      " -4.13741404e-03  3.61483321e-02  1.63200632e-01  6.66958839e-03\n",
      "  2.60019507e-02 -7.54401088e-02  1.05744541e-01  2.26726439e-02\n",
      "  1.52273968e-01  9.16861184e-03  5.90371601e-02  1.70791857e-02\n",
      "  1.53799606e-02 -4.36023585e-02  1.43229002e-02  3.61035801e-02\n",
      "  5.11723943e-03  5.59067689e-02  1.40079588e-01  6.59587681e-02\n",
      "  1.21263713e-01  1.23182992e-02 -1.25122771e-01 -6.98152930e-02\n",
      "  3.98342758e-02  8.59933197e-02 -5.02765412e-04  6.01728670e-02\n",
      " -5.62067004e-03 -6.48738444e-02 -7.13242516e-02 -1.50332376e-01\n",
      " -6.78230962e-03 -2.39847377e-02 -1.11709192e-01 -2.47558039e-02\n",
      " -8.33218079e-03  6.07139356e-02  1.71000045e-02 -2.67484020e-02\n",
      " -7.81001383e-03 -1.61248952e-01 -1.72361508e-01 -5.03190383e-02\n",
      "  4.42450978e-02  8.17927569e-02 -2.42710654e-02  2.79136132e-02\n",
      "  1.64889142e-01  6.94844276e-02 -5.98980077e-02 -7.73472637e-02\n",
      " -4.54744510e-03  1.96650252e-02  6.86764196e-02  3.36508527e-02\n",
      "  9.74901840e-02 -1.36354581e-01 -1.37143537e-01 -8.29068050e-02\n",
      " -4.97464500e-02 -1.37518436e-01  1.23429544e-01 -5.27292006e-02\n",
      "  5.81390597e-02  1.58086028e-02  1.18738852e-01 -3.70388404e-02\n",
      "  1.46673098e-01  4.08348329e-02 -1.36722475e-01  6.21895008e-02\n",
      "  2.75331661e-02 -8.89034197e-02  3.42041925e-02  1.38122141e-01\n",
      "  1.02407083e-01  1.56485885e-01 -1.66399591e-02  2.28147954e-04\n",
      " -4.54687327e-02 -1.45206183e-01  7.26639181e-02  9.33875069e-02\n",
      "  6.65738434e-02  6.75462128e-04  4.09405865e-02  1.49025796e-02\n",
      "  8.68486539e-02 -5.28802685e-02 -3.35600749e-02  3.79362032e-02\n",
      "  1.13384888e-01  1.43507225e-02  1.15575932e-01  9.86037925e-02\n",
      " -1.56402513e-02 -2.54230380e-01 -1.63178868e-03  1.01723753e-01\n",
      " -2.93041058e-02  4.05629873e-02  1.90591067e-01  5.08608762e-03\n",
      "  8.47278014e-02  1.51590392e-01  1.06653474e-01  5.82639500e-02\n",
      "  1.64566532e-01  1.17833376e-01 -1.17120557e-01 -1.72228999e-02\n",
      " -1.03781477e-01  1.00955190e-02 -3.42182964e-02 -2.26317197e-02\n",
      "  1.90963119e-01 -1.50392242e-02  1.76171556e-01  4.99808714e-02\n",
      " -7.41236508e-02  7.23484010e-02  9.21997353e-02  2.76771728e-02\n",
      " -2.28375360e-01  3.91506627e-02  5.40277502e-03 -1.37227671e-02\n",
      "  5.65733053e-02 -7.43224025e-02 -1.52114198e-01 -1.49210185e-01\n",
      "  5.40534183e-02  5.19941039e-02 -2.63945311e-02 -8.43986645e-02\n",
      "  7.53029212e-02  1.36285648e-01 -1.98219419e-02  7.45689720e-02\n",
      "  1.17790148e-01  1.02239832e-01  1.29585803e-01 -4.14237902e-02\n",
      "  6.68795183e-02  1.32117704e-01  2.15681829e-03 -1.27839278e-02]\n",
      "[-0.13942023  0.0137959  -0.06740705 -0.07502398  0.08166495 -0.05542218\n",
      "  0.1150433  -0.08661223 -0.02022294 -0.04211183  0.01140482 -0.12122489\n",
      "  0.05693653  0.07203142 -0.02006962 -0.01657444 -0.15714972  0.05166233\n",
      " -0.09145444  0.1754596  -0.00451972  0.091703    0.07381586  0.01852003\n",
      "  0.01345275  0.0829929  -0.02438819 -0.03504184  0.11880571  0.10887939\n",
      "  0.11945616  0.03056883 -0.00987958 -0.10234129  0.15779011 -0.03277857\n",
      " -0.06849214 -0.13212182 -0.05876553 -0.13597606  0.14650871 -0.09745727\n",
      " -0.05653883  0.07255869  0.08939698  0.01775063  0.17691927 -0.18910365\n",
      " -0.08000818  0.01014439 -0.07561215 -0.18680422 -0.05365489  0.08249871\n",
      "  0.08311456  0.04433785  0.00108168 -0.11220922  0.08188035  0.09894659\n",
      "  0.03767698  0.00936386  0.07056355  0.07240058  0.04393752  0.03626012\n",
      " -0.0323587   0.18853424 -0.0230919   0.01273701 -0.07375882 -0.11915132\n",
      " -0.01263567 -0.07830048 -0.04454889 -0.00822497  0.02948404  0.14790456\n",
      " -0.05342856 -0.09128951  0.05860143  0.07711332 -0.07506794  0.01062502\n",
      "  0.10293601 -0.07712085 -0.07008394  0.02424959 -0.01001131 -0.11712424\n",
      "  0.0282899   0.0224398   0.00702077 -0.01507641 -0.01007874 -0.08048052\n",
      "  0.12768033  0.15778574  0.04520855 -0.06507161 -0.07563092 -0.11287592\n",
      "  0.07777029  0.04876852 -0.17980288 -0.03011498 -0.09996927  0.07843379\n",
      "  0.02364383  0.00827417  0.01243344  0.03921394  0.09773367  0.0111016\n",
      "  0.02599917  0.00778995 -0.01285358  0.05573699 -0.15475398 -0.0258024\n",
      "  0.0066073  -0.13314754  0.12635262 -0.01527471  0.00772398  0.16880564\n",
      " -0.03133807 -0.11432968  0.15274219 -0.08909588 -0.04438321 -0.06100288\n",
      " -0.08601353 -0.07732104 -0.0238486  -0.00930452 -0.0704316   0.04107608\n",
      "  0.00452094  0.1067577  -0.03496384 -0.02863983 -0.14147277 -0.07158273\n",
      "  0.03342576  0.154254   -0.10643438  0.20999822  0.01514065  0.12414142\n",
      "  0.1643675   0.03385692  0.00584346  0.03423806  0.03647923 -0.0193457\n",
      " -0.16234869 -0.02345939 -0.04089504 -0.02452612  0.10163757  0.05113003\n",
      "  0.05781864  0.02344983 -0.01983573  0.0762413   0.00976504  0.10938346\n",
      " -0.16507481  0.0563136   0.00928871  0.00241581  0.07579398  0.25184867\n",
      "  0.02470408  0.01325214 -0.15950184 -0.08652438 -0.07490601 -0.07698312\n",
      " -0.00155391 -0.01940157 -0.11226479 -0.00542052  0.0879134   0.1483189\n",
      " -0.1198352  -0.05984928  0.08145422 -0.03600626  0.08530717 -0.05884254\n",
      "  0.02665452  0.01431102 -0.05848288 -0.15218844  0.04737469 -0.01040536\n",
      "  0.00770586 -0.1610737   0.0807339   0.02570181 -0.11402299  0.04187213\n",
      "  0.00765672 -0.07622384  0.17564495 -0.02919935  0.05691396 -0.09296321\n",
      " -0.01233161 -0.07313225  0.01100931 -0.03496176  0.00226601  0.07938448\n",
      "  0.01320335 -0.01098682 -0.10245686 -0.07005046 -0.02779014  0.0358294\n",
      " -0.09176941 -0.0747429   0.02999191 -0.1662973  -0.06182544  0.08026514\n",
      "  0.14941503  0.12469028 -0.10205218  0.08544198  0.00738829  0.0121614\n",
      "  0.0126956   0.09041988  0.04825181  0.10141857  0.02579628  0.07444274\n",
      " -0.02528876 -0.09308491  0.18866438  0.03860394 -0.06634     0.03484336\n",
      "  0.07277886 -0.0443944  -0.12126882  0.07735352  0.07631909  0.08469091\n",
      "  0.04691409 -0.03448593 -0.1226899   0.05094974]\n",
      "Waiting for JIT...\n",
      "0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_and_evaluate(get_config(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, workdir)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mmax_steps):\n\u001b[1;32m     22\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(res_sampler)\n\u001b[0;32m---> 23\u001b[0m     model\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstep(model\u001b[38;5;241m.\u001b[39mstate, batch)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mweighting\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mntk\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m config\u001b[38;5;241m.\u001b[39mweighting\u001b[38;5;241m.\u001b[39mupdate_every_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 137\u001b[0m, in \u001b[0;36mPINN.step\u001b[0;34m(self, state, batch, *args)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@partial\u001b[39m(pmap, axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, static_broadcasted_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, batch, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 137\u001b[0m     grads \u001b[38;5;241m=\u001b[39m grad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss)(state\u001b[38;5;241m.\u001b[39mparams, state\u001b[38;5;241m.\u001b[39mweights, batch, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    138\u001b[0m     grads \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mpmean(grads, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n",
      "    \u001b[0;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 102\u001b[0m, in \u001b[0;36mPINN.loss\u001b[0;34m(self, params, weights, batch, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, weights, batch, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses(params, batch, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Compute weighted loss\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     weighted_losses \u001b[38;5;241m=\u001b[39m tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m*\u001b[39m y, losses, weights)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mCaseZero.losses\u001b[0;34m(self, params, batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlosses\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Initial condition loss\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     u_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_net(params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt0) \u001b[38;5;66;03m# -------------- DEBUG\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     ics_loss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu0 \u001b[38;5;241m-\u001b[39m u_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Residual loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mCaseZero.u_net\u001b[0;34m(self, params, t)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mu_net\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, t):\n\u001b[0;32m---> 18\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mapply_fn(params, t) \u001b[38;5;66;03m# -------------- DEBUG\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/jaxpi/archs.py:152\u001b[0m, in \u001b[0;36mMlp.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m     x \u001b[38;5;241m=\u001b[39m FourierEmbs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_emb)(x)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 152\u001b[0m     x \u001b[38;5;241m=\u001b[39m Dense(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, reparam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparam)(x)\n\u001b[1;32m    153\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(x)\n\u001b[1;32m    155\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim, reparam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparam)(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/jaxpi/archs.py:105\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparam \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m    104\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_init, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparam[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_fact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    109\u001b[0m     g, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         _weight_fact(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m         (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures),\n\u001b[1;32m    117\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(get_config(), 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
